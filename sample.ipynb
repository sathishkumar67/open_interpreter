{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83d7c83d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mss in c:\\users\\sathish\\miniconda3\\envs\\py3.10\\lib\\site-packages (from -r requirements.txt (line 1)) (10.1.0)\n",
      "Collecting boto3 (from -r requirements.txt (line 2))\n",
      "  Downloading boto3-1.40.52-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting botocore<1.41.0,>=1.40.52 (from boto3->-r requirements.txt (line 2))\n",
      "  Downloading botocore-1.40.52-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->-r requirements.txt (line 2))\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3->-r requirements.txt (line 2))\n",
      "  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\sathish\\miniconda3\\envs\\py3.10\\lib\\site-packages (from botocore<1.41.0,>=1.40.52->boto3->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\sathish\\miniconda3\\envs\\py3.10\\lib\\site-packages (from botocore<1.41.0,>=1.40.52->boto3->-r requirements.txt (line 2)) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sathish\\miniconda3\\envs\\py3.10\\lib\\site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.52->boto3->-r requirements.txt (line 2)) (1.17.0)\n",
      "Downloading boto3-1.40.52-py3-none-any.whl (139 kB)\n",
      "Downloading botocore-1.40.52-py3-none-any.whl (14.1 MB)\n",
      "   ---------------------------------------- 0.0/14.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/14.1 MB 4.8 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.8/14.1 MB 4.8 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.9/14.1 MB 4.8 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.9/14.1 MB 4.8 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 5.0/14.1 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 6.0/14.1 MB 4.8 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 7.1/14.1 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 8.1/14.1 MB 4.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 9.2/14.1 MB 4.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 10.2/14.1 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 11.0/14.1 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 12.1/14.1 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 13.1/14.1 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.9/14.1 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.1/14.1 MB 4.6 MB/s eta 0:00:00\n",
      "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
      "\n",
      "   ---------- ----------------------------- 1/4 [botocore]\n",
      "   ---------- ----------------------------- 1/4 [botocore]\n",
      "   ---------- ----------------------------- 1/4 [botocore]\n",
      "   ---------- ----------------------------- 1/4 [botocore]\n",
      "   ---------- ----------------------------- 1/4 [botocore]\n",
      "   ---------- ----------------------------- 1/4 [botocore]\n",
      "   ---------- ----------------------------- 1/4 [botocore]\n",
      "   ---------- ----------------------------- 1/4 [botocore]\n",
      "   ---------- ----------------------------- 1/4 [botocore]\n",
      "   ---------- ----------------------------- 1/4 [botocore]\n",
      "   ---------- ----------------------------- 1/4 [botocore]\n",
      "   ---------- ----------------------------- 1/4 [botocore]\n",
      "   ---------- ----------------------------- 1/4 [botocore]\n",
      "   ---------- ----------------------------- 1/4 [botocore]\n",
      "   ---------- ----------------------------- 1/4 [botocore]\n",
      "   ---------- ----------------------------- 1/4 [botocore]\n",
      "   ---------- ----------------------------- 1/4 [botocore]\n",
      "   ---------- ----------------------------- 1/4 [botocore]\n",
      "   ---------- ----------------------------- 1/4 [botocore]\n",
      "   ---------- ----------------------------- 1/4 [botocore]\n",
      "   ---------- ----------------------------- 1/4 [botocore]\n",
      "   ---------- ----------------------------- 1/4 [botocore]\n",
      "   ---------- ----------------------------- 1/4 [botocore]\n",
      "   ---------- ----------------------------- 1/4 [botocore]\n",
      "   ---------- ----------------------------- 1/4 [botocore]\n",
      "   ---------- ----------------------------- 1/4 [botocore]\n",
      "   ---------- ----------------------------- 1/4 [botocore]\n",
      "   ---------- ----------------------------- 1/4 [botocore]\n",
      "   -------------------- ------------------- 2/4 [s3transfer]\n",
      "   ------------------------------ --------- 3/4 [boto3]\n",
      "   ------------------------------ --------- 3/4 [boto3]\n",
      "   ---------------------------------------- 4/4 [boto3]\n",
      "\n",
      "Successfully installed boto3-1.40.52 botocore-1.40.52 jmespath-1.0.1 s3transfer-0.14.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2beca7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpreter import interpreter\n",
    "from interpreter.utils import take_screenshot\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e2cd7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert GUI automation agent that performs tasks in web browsers. \n",
    "You analyze screenshots and user queries to determine the exact sequence of actions needed.\n",
    "You can only interact with the GUI using keyboard actions. So use keyboard shortcuts to navigate and perform tasks.\n",
    "When typing mention where you are typing (e.g., address bar, search box, form field).\n",
    "Process Flow:\n",
    "    1. Analyze the current screenshot and query to understand the task\n",
    "    2. Reason step-by-step about required actions to take based on the current screen state.\n",
    "    3. After reasoning all the steps, you will describe and execute one action by outputting a JSON object with the required action.\n",
    "    4. Wait for a new screenshot to be provided. Use this screenshot to check if the action was successful before continuing with the next action. If the action was not successful, adjust your approach based on the new screenshot.\n",
    "    5. Output plain text when keyboard actions are not required.\n",
    "    6. Repeat steps 1-5 until the task is complete.\n",
    "    7. Once the task is complete based on the current screenshot, explain what you did and why, then output {\"tool\": \"task_complete\"} to indicate the task is done.\n",
    "Output Format:\n",
    "    1. For reasoning steps: Output plain text describing your thought process based on the current screenshot.\n",
    "    2. For keyboard actions: Output description of the action being taken based on the current screenshot and the python code to execute the action based on the current screenshot.\n",
    "program: \n",
    "    1. Use the pyautogui library to perform keyboard actions. \n",
    "    2. Import pyautogui at the start of your program.\n",
    "    3. Use pyautogui.hotkey() for keyboard shortcuts. \n",
    "    4. When typing go to the appropriate field first (e.g., address bar) then use pyautogui.write() to type.\n",
    "Important Notes:\n",
    "    1. Always analyze the current screenshot before taking any action.\n",
    "    2. If an action does not lead to the expected result of the task, adjust your approach based on the new screenshot.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e26d00bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.llm.model = \"azure/gpt-4.1\"\n",
    "interpreter.llm.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "interpreter.llm.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "interpreter.llm.api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "interpreter.auto_run = True\n",
    "interpreter.system_message = SYSTEM_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7eac7309",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'role': 'assistant',\n",
       "  'type': 'message',\n",
       "  'content': \"I see that the current browser window already has multiple tabs open, and the active tab is a new Google homepage tab. To open a new tab using the keyboard, the standard shortcut is Ctrl + T.\\n\\nAction: I will press Ctrl + T to open a new tab.\\n\\n```python\\nimport pyautogui\\npyautogui.hotkey('ctrl', 't')\\n```\"}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query to ask the assistant\n",
    "query = \"open new tab\"\n",
    "# query = \"Create an excel sheet of 10 students 5 subject and give them the average score and percentage in new column.\"\n",
    "\n",
    "# Build the message with optional image\n",
    "message_with_image = [{\"role\": \"user\", \"type\": \"message\", \"content\": query},\n",
    "                      {\"role\": \"user\", \"type\": \"image\", \"format\": \"path\", \"content\": take_screenshot()}]\n",
    "\n",
    "interpreter.chat(message_with_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a76ebf57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'type': 'message', 'content': 'open new tab'},\n",
       " {'role': 'user',\n",
       "  'type': 'image',\n",
       "  'format': 'path',\n",
       "  'content': 'screenshots\\\\screenshot_20251014_202327.png'},\n",
       " {'role': 'assistant',\n",
       "  'type': 'message',\n",
       "  'content': 'I see that the browser window is open, and the user wants to open a new tab. To open a new tab using the keyboard, I will use the shortcut Ctrl + T.\\n\\nExecuting the action to open a new tab using Ctrl + T.\\n\\n{\\n  \"description\": \"Open a new tab in the browser using Ctrl + T\",\\n  \"code\": \"import pyautogui\\\\npyautogui.hotkey(\\'ctrl\\', \\'t\\')\"\\n}'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpreter.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b616b9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getenv(\"AZURE_OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8529b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.llm.model = \"azure/gpt-4.1\"\n",
    "interpreter.llm.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "interpreter.llm.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "interpreter.llm.api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "interpreter.auto_run = True\n",
    "interpreter.system_message = SYSTEM_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc05d02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8496fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">We were unable to determine the context window of this model.</span> Defaulting to 8000.                                  \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mWe were unable to determine the context window of this model.\u001b[0m Defaulting to 8000.                                  \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">If your model can handle more, run <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">self.context_window = {token limit}</span>.                                            \n",
       "</pre>\n"
      ],
      "text/plain": [
       "If your model can handle more, run \u001b[1;36;40mself.context_window = {token limit}\u001b[0m.                                            \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Also please set <span style=\"color: #008080; text-decoration-color: #008080; background-color: #000000; font-weight: bold\">self.max_tokens = {max tokens per response}</span>.                                                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Also please set \u001b[1;36;40mself.max_tokens = {max tokens per response}\u001b[0m.                                                       \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Continuing...                                                                                                      \n",
       "</pre>\n"
      ],
      "text/plain": [
       "Continuing...                                                                                                      \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "litellm.BadRequestError: BedrockException - b'{\"message\":\"Invocation of model ID anthropic.claude-sonnet-4-5-20250929-v1:0 with on-demand throughput isn\\xe2\\x80\\x99t supported. Retry your request with the ID or ARN of an inference profile that contains this model.\"}'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\sathish\\miniconda3\\envs\\py3.10\\lib\\site-packages\\litellm\\main.py:3160\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[0m\n\u001b[0;32m   3159\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconverse/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 3160\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mbedrock_converse_chat_completion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3165\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m   3167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use merged headers instead of original extra_headers\u001b[39;49;00m\n\u001b[0;32m   3171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3172\u001b[0m \u001b[43m        \u001b[49m\u001b[43macompletion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3176\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3177\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m bedrock_route \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconverse_like\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\sathish\\miniconda3\\envs\\py3.10\\lib\\site-packages\\litellm\\llms\\bedrock\\chat\\converse_handler.py:433\u001b[0m, in \u001b[0;36mBedrockConverseLLM.completion\u001b[1;34m(self, model, messages, api_base, custom_prompt_dict, model_response, encoding, logging_obj, optional_params, acompletion, timeout, litellm_params, logger_fn, extra_headers, client, api_key)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 433\u001b[0m     completion_stream \u001b[38;5;241m=\u001b[39m \u001b[43mmake_sync_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclient\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHTTPHandler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxy_endpoint_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[0;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjson_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfake_stream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfake_stream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    448\u001b[0m     streaming_response \u001b[38;5;241m=\u001b[39m CustomStreamWrapper(\n\u001b[0;32m    449\u001b[0m         completion_stream\u001b[38;5;241m=\u001b[39mcompletion_stream,\n\u001b[0;32m    450\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    451\u001b[0m         custom_llm_provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbedrock\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    452\u001b[0m         logging_obj\u001b[38;5;241m=\u001b[39mlogging_obj,\n\u001b[0;32m    453\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\sathish\\miniconda3\\envs\\py3.10\\lib\\site-packages\\litellm\\llms\\bedrock\\chat\\converse_handler.py:37\u001b[0m, in \u001b[0;36mmake_sync_call\u001b[1;34m(client, api_base, headers, data, model, messages, logging_obj, json_mode, fake_stream)\u001b[0m\n\u001b[0;32m     35\u001b[0m     client \u001b[38;5;241m=\u001b[39m _get_httpx_client()  \u001b[38;5;66;03m# Create a new client if none provided\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfake_stream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\sathish\\miniconda3\\envs\\py3.10\\lib\\site-packages\\litellm\\llms\\custom_httpx\\http_handler.py:802\u001b[0m, in \u001b[0;36mHTTPHandler.post\u001b[1;34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[0m\n\u001b[0;32m    801\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code)\n\u001b[1;32m--> 802\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\sathish\\miniconda3\\envs\\py3.10\\lib\\site-packages\\litellm\\llms\\custom_httpx\\http_handler.py:784\u001b[0m, in \u001b[0;36mHTTPHandler.post\u001b[1;34m(self, url, data, json, params, headers, stream, timeout, files, content, logging_obj)\u001b[0m\n\u001b[0;32m    783\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39msend(req, stream\u001b[38;5;241m=\u001b[39mstream)\n\u001b[1;32m--> 784\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\sathish\\miniconda3\\envs\\py3.10\\lib\\site-packages\\httpx\\_models.py:829\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    828\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[1;32m--> 829\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPStatusError\u001b[0m: Client error '400 Bad Request' for url 'https://bedrock-runtime.us-east-1.amazonaws.com/model/anthropic.claude-sonnet-4-5-20250929-v1%3A0/converse-stream'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/400",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m interpreter\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mtemperature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Now you can start chatting with the model\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[43minterpreter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHello, how can you do?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sathish\\Downloads\\pytorch\\github_repos\\open_interpreter\\interpreter\\core\\core.py:191\u001b[0m, in \u001b[0;36mOpenInterpreter.chat\u001b[1;34m(self, message, display, stream, blocking)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming_chat(message\u001b[38;5;241m=\u001b[39mmessage, display\u001b[38;5;241m=\u001b[39mdisplay)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# If stream=False, *pull* from the stream.\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_streaming_chat(message\u001b[38;5;241m=\u001b[39mmessage, display\u001b[38;5;241m=\u001b[39mdisplay):\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# Return new messages\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sathish\\Downloads\\pytorch\\github_repos\\open_interpreter\\interpreter\\core\\core.py:223\u001b[0m, in \u001b[0;36mOpenInterpreter._streaming_chat\u001b[1;34m(self, message, display)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_streaming_chat\u001b[39m(\u001b[38;5;28mself\u001b[39m, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, display\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# Sometimes a little more code -> a much better experience!\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# Display mode actually runs interpreter.chat(display=False, stream=True) from within the terminal_interface.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# wraps the vanilla .chat(display=False) generator in a display.\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# Quite different from the plain generator stuff. So redirect to that\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m display:\n\u001b[1;32m--> 223\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m terminal_interface(\u001b[38;5;28mself\u001b[39m, message)\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;66;03m# One-off message\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sathish\\Downloads\\pytorch\\github_repos\\open_interpreter\\interpreter\\terminal_interface\\terminal_interface.py:162\u001b[0m, in \u001b[0;36mterminal_interface\u001b[1;34m(interpreter, message)\u001b[0m\n\u001b[0;32m    154\u001b[0m             message \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    155\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    156\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    157\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    158\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: image_path,\n\u001b[0;32m    159\u001b[0m             }\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m interpreter\u001b[38;5;241m.\u001b[39mchat(message, display\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    163\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;66;03m# Is this for thine eyes?\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sathish\\Downloads\\pytorch\\github_repos\\open_interpreter\\interpreter\\core\\core.py:259\u001b[0m, in \u001b[0;36mOpenInterpreter._streaming_chat\u001b[1;34m(self, message, display)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_messages_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessages)\n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# DISABLED because I think we should just not transmit images to non-multimodal models?\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# REENABLE this when multimodal becomes more common:\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    257\u001b[0m \n\u001b[0;32m    258\u001b[0m \u001b[38;5;66;03m# This is where it all happens!\u001b[39;00m\n\u001b[1;32m--> 259\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_respond_and_store()\n\u001b[0;32m    261\u001b[0m \u001b[38;5;66;03m# Save conversation if we've turned conversation_history on\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconversation_history:\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;66;03m# If it's the first message, set the conversation name\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sathish\\Downloads\\pytorch\\github_repos\\open_interpreter\\interpreter\\core\\core.py:318\u001b[0m, in \u001b[0;36mOpenInterpreter._respond_and_store\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    315\u001b[0m last_flag_base \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m respond(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;66;03m# For async usage\u001b[39;00m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop_event\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_event\u001b[38;5;241m.\u001b[39mis_set():\n\u001b[0;32m    321\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpen Interpreter stopping.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sathish\\Downloads\\pytorch\\github_repos\\open_interpreter\\interpreter\\core\\respond.py:87\u001b[0m, in \u001b[0;36mrespond\u001b[1;34m(interpreter)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m     84\u001b[0m     interpreter\u001b[38;5;241m.\u001b[39mmessages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     85\u001b[0m ):  \u001b[38;5;66;03m# If it is, we should run the code (we do below)\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 87\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m interpreter\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mrun(messages_for_llm):\n\u001b[0;32m     88\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mchunk}\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mBudgetExceededError:\n",
      "File \u001b[1;32mc:\\Users\\sathish\\Downloads\\pytorch\\github_repos\\open_interpreter\\interpreter\\core\\llm\\llm.py:324\u001b[0m, in \u001b[0;36mLlm.run\u001b[1;34m(self, messages)\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m run_tool_calling_llm(\u001b[38;5;28mself\u001b[39m, params)\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 324\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m run_text_llm(\u001b[38;5;28mself\u001b[39m, params)\n",
      "File \u001b[1;32mc:\\Users\\sathish\\Downloads\\pytorch\\github_repos\\open_interpreter\\interpreter\\core\\llm\\run_text_llm.py:20\u001b[0m, in \u001b[0;36mrun_text_llm\u001b[1;34m(llm, params)\u001b[0m\n\u001b[0;32m     17\u001b[0m accumulated_block \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m language \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mcompletions(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m llm\u001b[38;5;241m.\u001b[39minterpreter\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChunk in coding_llm\u001b[39m\u001b[38;5;124m\"\u001b[39m, chunk)\n",
      "File \u001b[1;32mc:\\Users\\sathish\\Downloads\\pytorch\\github_repos\\open_interpreter\\interpreter\\core\\llm\\llm.py:466\u001b[0m, in \u001b[0;36mfixed_litellm_completions\u001b[1;34m(**params)\u001b[0m\n\u001b[0;32m    463\u001b[0m             params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 466\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m first_error\n",
      "File \u001b[1;32mc:\\Users\\sathish\\Downloads\\pytorch\\github_repos\\open_interpreter\\interpreter\\core\\llm\\llm.py:443\u001b[0m, in \u001b[0;36mfixed_litellm_completions\u001b[1;34m(**params)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(attempts):\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 443\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mcompletion(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m  \u001b[38;5;66;03m# If the completion is successful, exit the function\u001b[39;00m\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\sathish\\miniconda3\\envs\\py3.10\\lib\\site-packages\\litellm\\utils.py:1356\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1352\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[0;32m   1353\u001b[0m     logging_obj\u001b[38;5;241m.\u001b[39mfailure_handler(\n\u001b[0;32m   1354\u001b[0m         e, traceback_exception, start_time, end_time\n\u001b[0;32m   1355\u001b[0m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[1;32m-> 1356\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32mc:\\Users\\sathish\\miniconda3\\envs\\py3.10\\lib\\site-packages\\litellm\\utils.py:1231\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1229\u001b[0m         print_verbose(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[1;32m-> 1231\u001b[0m result \u001b[38;5;241m=\u001b[39m original_function(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1232\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m   1233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[0;32m   1234\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m   1235\u001b[0m     call_type\u001b[38;5;241m=\u001b[39mcall_type,\n\u001b[0;32m   1236\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\sathish\\miniconda3\\envs\\py3.10\\lib\\site-packages\\litellm\\main.py:3733\u001b[0m, in \u001b[0;36mcompletion\u001b[1;34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[0m\n\u001b[0;32m   3730\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[0;32m   3731\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   3732\u001b[0m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[1;32m-> 3733\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3734\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3735\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3736\u001b[0m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3737\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3739\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sathish\\miniconda3\\envs\\py3.10\\lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:2273\u001b[0m, in \u001b[0;36mexception_type\u001b[1;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[0;32m   2271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[0;32m   2272\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlitellm_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, litellm_response_headers)\n\u001b[1;32m-> 2273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m   2274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2275\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mLITELLM_EXCEPTION_TYPES:\n",
      "File \u001b[1;32mc:\\Users\\sathish\\miniconda3\\envs\\py3.10\\lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:994\u001b[0m, in \u001b[0;36mexception_type\u001b[1;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m original_exception\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[0;32m    993\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 994\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequestError(\n\u001b[0;32m    995\u001b[0m         message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBedrockException - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_exception\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    996\u001b[0m         llm_provider\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbedrock\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    997\u001b[0m         model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    998\u001b[0m         response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    999\u001b[0m     )\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m original_exception\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[0;32m   1001\u001b[0m     exception_mapping_worked \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mBadRequestError\u001b[0m: litellm.BadRequestError: BedrockException - b'{\"message\":\"Invocation of model ID anthropic.claude-sonnet-4-5-20250929-v1:0 with on-demand throughput isn\\xe2\\x80\\x99t supported. Retry your request with the ID or ARN of an inference profile that contains this model.\"}'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from interpreter import OpenInterpreter\n",
    "\n",
    "interpreter = OpenInterpreter()\n",
    "\n",
    "# Set your AWS credentials and region\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"AKIA46E27RDMQAQAVUX2\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"LD/Eazu01Z4XVYyngo2kVbqNrht+YxYt/gu0JhvP\"\n",
    "os.environ[\"AWS_REGION_NAME\"] = \"us-east-1\"\n",
    "\n",
    "interpreter.auto_run = True\n",
    "\n",
    "# Set the model to use from AWS Bedrock\n",
    "interpreter.llm.model = \"bedrock/anthropic.claude-sonnet-4-5-20250929-v1:0\"\n",
    "\n",
    "# You can optionally set other parameters\n",
    "interpreter.llm.max_tokens = 2048\n",
    "interpreter.llm.temperature = 0.7\n",
    "\n",
    "# Now you can start chatting with the model\n",
    "interpreter.chat(\"Hello, how can you do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4176bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Can't invoke 'bedrock/anthropic.claude-sonnet-4-5-20250929-v1:0'. Reason: An error occurred (ValidationException) when calling the Converse operation: The provided model identifier is invalid.\n"
     ]
    }
   ],
   "source": [
    "# Use the Conversation API to send a text message to Anthropic Claude.\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Create a Bedrock Runtime client in the AWS Region you want to use.\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "\n",
    "# Set the model ID, e.g., Claude 3 Haiku.\n",
    "model_id = \"anthropic.claude-sonnet-4-5-20250929-v1:0\"\n",
    "\n",
    "# Start a conversation with the user message.\n",
    "user_message = \"Describe the purpose of a 'hello world' program in one line.\"\n",
    "conversation = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": user_message}],\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Send the message to the model, using a basic inference configuration.\n",
    "    response = client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=conversation,\n",
    "        inferenceConfig={\"maxTokens\": 512, \"temperature\": 0.5, \"topP\": 0.9},\n",
    "    )\n",
    "\n",
    "    # Extract and print the response text.\n",
    "    response_text = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    print(response_text)\n",
    "\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b73241",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
